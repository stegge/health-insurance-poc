{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c03b15-d914-43c9-8b91-080c5a8e06fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pandas import *\n",
    "date = datetime.today().strftime('%Y_%m_%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fae9cc-a75d-48d8-ac20-c75a45145b35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Layer Table Name Bronze Table Map"
    }
   },
   "outputs": [],
   "source": [
    "tables = {}\n",
    "table_list = spark.sql(\"SHOW TABLES IN workspace.`tegge-insurance-data`\").toPandas()\n",
    "for table in table_list['tableName']:\n",
    "    if table.endswith('_bronze'):\n",
    "        key = table.replace('_bronze', '_silver')\n",
    "    else:\n",
    "        key = table\n",
    "    tables[key] = f\"workspace.`tegge-insurance-data`.{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974dc0a5-5288-49dd-9bdf-f4eb4f37cd7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Table and Primary Key Dictionary"
    }
   },
   "outputs": [],
   "source": [
    "pk_dict = spark.read.format(\"delta\").table(\"workspace.`tegge-insurance-data-silver`.table_pk_fk\")\n",
    "\n",
    "pk_dict = pk_dict.select(\"Table_Name\", \"Primary_Key\").toPandas()\n",
    "\n",
    "pk_dict = dict(zip(pk_dict[\"Table_Name\"] , pk_dict[\"Primary_Key\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea640684-5751-4276-b763-5a3a3db81284",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Table and Foreign Key Dictionary"
    }
   },
   "outputs": [],
   "source": [
    "fk_dict = spark.read.format(\"delta\").table(\"workspace.`tegge-insurance-data-silver`.table_pk_fk\")\n",
    "\n",
    "fk_dict = fk_dict.select(\"Table_Name\", \"Foreign_Key\").toPandas()\n",
    "\n",
    "fk_dict = dict(zip(fk_dict[\"Table_Name\"] , fk_dict[\"Foreign_Key\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e16f10-dad2-46ed-87d9-14870a313a50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load filtered Delta tables to dfs"
    }
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for df_name, table in tables.items():\n",
    "    dfs[df_name] = spark.read.format(\"delta\").table(table).where(f\"load_timestamp LIKE '%{date}%'\").drop(\"source\", \"load_timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c209acc-2af8-4560-b63b-9d671f00c515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfs[\"pharmacy_claims_silver\"] = dfs[\"pharmacy_claims_silver\"].withColumnRenamed(\"rx_claim_id\", \"pharmacy_claim_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c22215d-8f3c-4529-985d-f56024fa11fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a dictionary of the current schema"
    }
   },
   "outputs": [],
   "source": [
    "df_schemas = {}\n",
    "for df_name, df in dfs.items():\n",
    "    try:\n",
    "        df_schemas[df_name] = df.schema.fieldNames()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read schema for {df_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0255555a-63ad-403b-b633-73c6f04527c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify and Drop Null PK/FK Rows"
    }
   },
   "outputs": [],
   "source": [
    "# Process all tables in dfs\n",
    "for table_name, df in dfs.items():\n",
    "    # Step 1: Identify rows with null primary key\n",
    "    pk_col = pk_dict.get(table_name)\n",
    "    if pk_col:\n",
    "        null_pk_df = df.filter(col(pk_col).isNull())\n",
    "        globals()[f\"{table_name}_null_pks\"] = null_pk_df\n",
    "        df = df.filter(col(pk_col).isNotNull())\n",
    "    \n",
    "    # Step 2: Identify rows with null foreign key\n",
    "    fk_col = fk_dict.get(table_name)\n",
    "    if fk_col:\n",
    "        null_fk_df = df.filter(col(fk_col).isNull())\n",
    "        globals()[f\"{table_name}_null_fks\"] = null_fk_df\n",
    "        #df = df.filter(col(fk_col).isNotNull())\n",
    "    \n",
    "    # Update dfs with cleaned DataFrame\n",
    "    dfs[table_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b14e41-00db-4666-8db1-52851454da02",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Null PK and FK Records for Investigation"
    }
   },
   "outputs": [],
   "source": [
    "for table_name in dfs.keys():\n",
    "    null_pk_df = globals().get(f\"{table_name}_null_pks\")\n",
    "    if null_pk_df is not None and null_pk_df.count() > 0:\n",
    "        null_pk_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"workspace.`tegge-insurance-data-anomalies`.null_pk_{table_name}_{date}\")\n",
    "    # null_fk_df = globals().get(f\"{table_name}_null_fks\")\n",
    "    # if null_fk_df is not None and null_fk_df.count() > 0:\n",
    "    #     null_fk_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"workspace.`tegge-insurance-data-anomalies`.null_fk_{table_name}_{date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0cb0c0-5dd3-40af-a929-141f6e5eecc5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update any null employer_id to Individual"
    }
   },
   "outputs": [],
   "source": [
    "for table_name, df in dfs.items():\n",
    "    if \"employer_id\" in df_schemas[table_name]:\n",
    "        df = df.fillna({\"employer_id\": 9999})\n",
    "    dfs[table_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4dd706-4e80-4107-bc03-faf6bc0f40fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicate_pk_tables = {}\n",
    "\n",
    "for table_name, df in dfs.items():\n",
    "    pk_col = pk_dict.get(table_name)\n",
    "    if pk_col and pk_col in df.columns:\n",
    "        dup_df = df.groupBy(pk_col).count().filter(col(\"count\") > 1)\n",
    "        if dup_df.count() > 0:\n",
    "            duplicate_pk_tables[table_name] = dup_df\n",
    "            print(f\"Duplicates found in {table_name} based on primary key: {pk_col}\")\n",
    "        else:\n",
    "            print(f\"No duplicates in {table_name} based on primary key: {pk_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae84908c-312a-4398-acf3-b637ed2b4b61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Email Support if null keys exist"
    }
   },
   "outputs": [],
   "source": [
    "# import smtplib\n",
    "# from email.message import EmailMessage\n",
    "\n",
    "# def send_alert(subject, body, to_email):\n",
    "#     msg = EmailMessage()\n",
    "#     msg.set_content(body)\n",
    "#     msg['Subject'] = subject\n",
    "#     msg['From'] = 'your_email@example.com'\n",
    "#     msg['To'] = to_email\n",
    "#     with smtplib.SMTP('smtp.yourprovider.com') as s:\n",
    "#         s.send_message(msg)\n",
    "\n",
    "# alert_needed = False\n",
    "# alert_tables = []\n",
    "# for table_name in dfs.keys():\n",
    "#     if globals().get(f\"{table_name}_null_pks\") is not None and globals()[f\"{table_name}_null_pks\"].count() > 0:\n",
    "#         alert_needed = True\n",
    "#         alert_tables.append(f\"{table_name} (null PK)\")\n",
    "#     if globals().get(f\"{table_name}_null_fks\") is not None and globals()[f\"{table_name}_null_fks\"].count() > 0:\n",
    "#         alert_needed = True\n",
    "#         alert_tables.append(f\"{table_name} (null FK)\")\n",
    "\n",
    "# if alert_needed:\n",
    "#     send_alert(\n",
    "#         subject=\"Null PK/FK Detected in Silver Tables\",\n",
    "#         body=f\"Tables with null PK/FK: {', '.join(alert_tables)}\",\n",
    "#         to_email=\"analyst@example.com\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0458c461-961a-4b58-8e16-1462a2f6a5dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kill the process if null keys exist"
    }
   },
   "outputs": [],
   "source": [
    "# # Check for null PK/FK and stop execution if any are found\n",
    "# stop_execution = False\n",
    "# stop_tables = []\n",
    "# for table_name in dfs.keys():\n",
    "#     null_pk_df = globals().get(f\"{table_name}_null_pks\")\n",
    "#     if null_pk_df is not None and null_pk_df.count() > 0:\n",
    "#         stop_execution = True\n",
    "#         stop_tables.append(f\"{table_name} (null PK)\")\n",
    "\n",
    "# if stop_execution:\n",
    "#     raise RuntimeError(f\"Null PK/FK detected in tables: {', '.join(stop_tables)}. Notebook execution stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7d6c73-d188-4916-96f6-80dd6f04bf76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update PK and FK columns to Non-Nullable"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "for table_name, df in dfs.items():\n",
    "    pk_col = pk_dict.get(table_name)\n",
    "    #fk_col = fk_dict.get(table_name)\n",
    "    # Get current schema\n",
    "    schema = df.schema\n",
    "    new_fields = []\n",
    "    for field in schema.fields:\n",
    "        if field.name == pk_col: #or field.name == fk_col:\n",
    "            new_fields.append(StructField(field.name, field.dataType, nullable=False))\n",
    "        else:\n",
    "            new_fields.append(field)\n",
    "    new_schema = StructType(new_fields)\n",
    "    # Recreate DataFrame with new schema\n",
    "    df_nonnull = spark.createDataFrame(df.rdd, schema=new_schema)\n",
    "    # Update dfs with new DataFrame\n",
    "    dfs[table_name] = df_nonnull"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "health_ins_poc_full_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
