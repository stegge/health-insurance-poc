{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c03b15-d914-43c9-8b91-080c5a8e06fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pandas import *\n",
    "import pandas as pd\n",
    "date = datetime.today().strftime('%Y_%m_%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fae9cc-a75d-48d8-ac20-c75a45145b35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Maps Silver Table to Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "tables = {}\n",
    "table_list = spark.sql(\"SHOW TABLES IN workspace.`tegge-insurance-data`\").toPandas()\n",
    "for table in table_list['tableName']:\n",
    "    if table.endswith('_bronze'):\n",
    "        key = table.replace('_bronze', '_silver')\n",
    "    else:\n",
    "        key = table\n",
    "    tables[key] = f\"workspace.`tegge-insurance-data`.{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e16f10-dad2-46ed-87d9-14870a313a50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load filtered Delta tables to dfs"
    }
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for df_name, table in tables.items():\n",
    "    dfs[df_name] = spark.read.format(\"delta\").table(table).where(f\"load_timestamp LIKE '%{date}%'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c209acc-2af8-4560-b63b-9d671f00c515",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standardize column names"
    }
   },
   "outputs": [],
   "source": [
    "dfs[\"pharmacy_claims_silver\"] = dfs[\"pharmacy_claims_silver\"].withColumnRenamed(\"rx_claim_id\", \"pharmacy_claim_id\")\n",
    "dfs[\"gl_transactions_silver\"] = dfs[\"gl_transactions_silver\"].withColumnRenamed(\"gl_txn_id\", \"gl_transaction_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c22215d-8f3c-4529-985d-f56024fa11fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a dictionary of the current schemas"
    }
   },
   "outputs": [],
   "source": [
    "df_schemas = {}\n",
    "for df_name, df in dfs.items():\n",
    "    try:\n",
    "        df_schemas[df_name] = df.schema.fieldNames()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read schema for {df_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0cb0c0-5dd3-40af-a929-141f6e5eecc5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update any null employer_id to 9999"
    }
   },
   "outputs": [],
   "source": [
    "for table_name, df in dfs.items():\n",
    "    if \"employer_id\" in df_schemas[table_name]:\n",
    "        df = df.fillna({\"employer_id\": 9999})\n",
    "    dfs[table_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974dc0a5-5288-49dd-9bdf-f4eb4f37cd7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Table:Primary Key Dictionary"
    }
   },
   "outputs": [],
   "source": [
    "pk_dict = spark.read.format(\"delta\").table(\"workspace.`tegge-insurance-data-silver`.table_pk_fk\")\n",
    "\n",
    "pk_dict = pk_dict.select(\"Table_Name\", \"Primary_Key\").toPandas()\n",
    "\n",
    "pk_dict = dict(zip(pk_dict[\"Table_Name\"] , pk_dict[\"Primary_Key\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea640684-5751-4276-b763-5a3a3db81284",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Table:Foreign Key Dictionary"
    }
   },
   "outputs": [],
   "source": [
    "fk_dict_df = spark.read.format(\"delta\").table(\"workspace.`tegge-insurance-data-silver`.table_pk_fk\")\n",
    "\n",
    "fk_dict_df = fk_dict_df.select(\"Table_Name\", \"Foreign_Key_1\", \"Foreign_Key_2\", \"Foreign_Key_3\", \"Foreign_Key_4\", \"Foreign_Key_5\").toPandas()\n",
    "\n",
    "fk_dict = {}\n",
    "for idx, row in fk_dict_df.iterrows():\n",
    "    # Collect all non-null, non-empty foreign keys for this table\n",
    "    fks = [row[f\"Foreign_Key_{i}\"] for i in range(1, 6) if pd.notnull(row[f\"Foreign_Key_{i}\"]) and str(row[f\"Foreign_Key_{i}\"]).strip() != \"\"]\n",
    "    fk_dict[row[\"Table_Name\"]] = fks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0255555a-63ad-403b-b633-73c6f04527c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify and Drop Null PK/FK Rows"
    }
   },
   "outputs": [],
   "source": [
    "# Process all tables in dfs\n",
    "for table_name, df in dfs.items():\n",
    "    # Step 1: Identify rows with null primary key\n",
    "    pk_col = pk_dict.get(table_name)\n",
    "    if pk_col:\n",
    "        null_pk_df = df.filter(col(pk_col).isNull())\n",
    "        globals()[f\"{table_name}_null_pks\"] = null_pk_df\n",
    "        df = df.filter(col(pk_col).isNotNull())\n",
    "    \n",
    "    # Step 2: Identify rows with null foreign key(s)\n",
    "    fk_cols = fk_dict.get(table_name)\n",
    "    if fk_cols:\n",
    "        for fk_col in fk_cols:\n",
    "            null_fk_df = df.filter(col(fk_col).isNull())\n",
    "            globals()[f\"{table_name}_null_fks_{fk_col}\"] = null_fk_df\n",
    "            #df = df.filter(col(fk_col).isNotNull())\n",
    "    \n",
    "    # Update dfs with cleaned DataFrame\n",
    "    dfs[table_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b14e41-00db-4666-8db1-52851454da02",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Null PK and FK Records for Investigation"
    }
   },
   "outputs": [],
   "source": [
    "for table_name in dfs.keys():\n",
    "    null_pk_df = globals().get(f\"{table_name}_null_pks\")\n",
    "    if null_pk_df is not None and null_pk_df.count() > 0:\n",
    "        null_pk_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"workspace.`tegge-insurance-data-anomalies`.null_pk_{table_name}_{date}\")\n",
    "    null_fk_df = globals().get(f\"{table_name}_null_fks\")\n",
    "    if null_fk_df is not None and null_fk_df.count() > 0:\n",
    "        null_fk_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"workspace.`tegge-insurance-data-anomalies`.null_fk_{table_name}_{date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9881ba7d-97a6-47b7-9ec1-ce04ac996c8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify and remove duplicates"
    }
   },
   "outputs": [],
   "source": [
    "# Read unique identifier mapping table\n",
    "unique_id_df = spark.read.format(\"delta\").table(\"workspace.`tegge-insurance-data-silver`.table_unique_identifier\")\n",
    "unique_id_pd = unique_id_df.toPandas()\n",
    "\n",
    "# Build mapping: table_name -> list of unique identifier columns\n",
    "unique_id_map = {}\n",
    "for idx, row in unique_id_pd.iterrows():\n",
    "    table = row['Table_Name']\n",
    "    # Collect all non-null, non-empty unique identifier columns\n",
    "    unique_cols = [row[f\"unique_identifier_{i}\"] for i in range(1, 6) if pd.notnull(row.get(f\"unique_identifier_{i}\")) and str(row.get(f\"unique_identifier_{i}\")).strip() != \"\"]\n",
    "    unique_id_map[table] = unique_cols\n",
    "\n",
    "# Identify duplicates using PK + unique identifiers, save to Delta tables, and remove from dfs\n",
    "duplicate_pkuid_tables = {}\n",
    "for table_name, df in dfs.items():\n",
    "    pk_col = pk_dict.get(table_name)\n",
    "    uid_cols = unique_id_map.get(table_name, [])\n",
    "    group_cols = []\n",
    "    if pk_col and pk_col in df.columns:\n",
    "        group_cols.append(pk_col)\n",
    "    for col_name in uid_cols:\n",
    "        if col_name in df.columns:\n",
    "            group_cols.append(col_name)\n",
    "    if group_cols:\n",
    "        dup_df = df.groupBy(*group_cols).count().filter(col(\"count\") > 1)\n",
    "        if dup_df.count() > 0:\n",
    "            # Join back to original DataFrame to get all duplicate records\n",
    "            dup_keys = dup_df.select(*group_cols)\n",
    "            dup_records = df.join(dup_keys, on=group_cols, how=\"inner\")\n",
    "            dup_records.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"workspace.`tegge-insurance-data-anomalies`.dup_{table_name}_{date}\")\n",
    "            print(f\"Duplicates saved for {table_name} based on PK+UID: {group_cols}\")\n",
    "            # Remove duplicates from DataFrame\n",
    "            df = df.dropDuplicates(group_cols)\n",
    "            dfs[table_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2e0a42-9940-4e13-87a9-f1f941331e99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Email Support if duplicate data exists"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53716b84-2c36-48ae-b21c-7e6657b4665a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kill the job if duplicate data exist"
    }
   },
   "outputs": [],
   "source": [
    "# # Check for duplicate records and stop execution if any are found\n",
    "# stop_execution = False\n",
    "# stop_tables = []\n",
    "# for table_name in dfs.keys():\n",
    "#     null_pk_df = globals().get(f\"{table_name}_null_pks\")\n",
    "#     if null_pk_df is not None and null_pk_df.count() > 0:\n",
    "#         stop_execution = True\n",
    "#         stop_tables.append(f\"{table_name} (null PK)\")\n",
    "\n",
    "# if stop_execution:\n",
    "#     raise RuntimeError(f\"Null PK/FK detected in tables: {', '.join(stop_tables)}. Notebook execution stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae84908c-312a-4398-acf3-b637ed2b4b61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Email Support if null keys exist"
    }
   },
   "outputs": [],
   "source": [
    "# import smtplib\n",
    "# from email.message import EmailMessage\n",
    "\n",
    "# def send_alert(subject, body, to_email):\n",
    "#     msg = EmailMessage()\n",
    "#     msg.set_content(body)\n",
    "#     msg['Subject'] = subject\n",
    "#     msg['From'] = 'your_email@example.com'\n",
    "#     msg['To'] = to_email\n",
    "#     with smtplib.SMTP('smtp.yourprovider.com') as s:\n",
    "#         s.send_message(msg)\n",
    "\n",
    "# alert_needed = False\n",
    "# alert_tables = []\n",
    "# for table_name in dfs.keys():\n",
    "#     if globals().get(f\"{table_name}_null_pks\") is not None and globals()[f\"{table_name}_null_pks\"].count() > 0:\n",
    "#         alert_needed = True\n",
    "#         alert_tables.append(f\"{table_name} (null PK)\")\n",
    "#     if globals().get(f\"{table_name}_null_fks\") is not None and globals()[f\"{table_name}_null_fks\"].count() > 0:\n",
    "#         alert_needed = True\n",
    "#         alert_tables.append(f\"{table_name} (null FK)\")\n",
    "\n",
    "# if alert_needed:\n",
    "#     send_alert(\n",
    "#         subject=\"Null PK/FK Detected in Silver Tables\",\n",
    "#         body=f\"Tables with null PK/FK: {', '.join(alert_tables)}\",\n",
    "#         to_email=\"analyst@example.com\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0458c461-961a-4b58-8e16-1462a2f6a5dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kill the process if null keys exist"
    }
   },
   "outputs": [],
   "source": [
    "# # Check for null PK/FK and stop execution if any are found\n",
    "# stop_execution = False\n",
    "# stop_tables = []\n",
    "# for table_name in dfs.keys():\n",
    "#     null_pk_df = globals().get(f\"{table_name}_null_pks\")\n",
    "#     if null_pk_df is not None and null_pk_df.count() > 0:\n",
    "#         stop_execution = True\n",
    "#         stop_tables.append(f\"{table_name} (null PK)\")\n",
    "\n",
    "# if stop_execution:\n",
    "#     raise RuntimeError(f\"Null PK/FK detected in tables: {', '.join(stop_tables)}. Notebook execution stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7d6c73-d188-4916-96f6-80dd6f04bf76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update PK and FK columns to Non-Nullable"
    }
   },
   "outputs": [],
   "source": [
    "# PySpark does not support changing column nullability directly in-place on serverless compute.\n",
    "# The best workaround is to use SQL ALTER TABLE statements to set columns as NOT NULL.\n",
    "for table_name, df in dfs.items():\n",
    "    pk_col = pk_dict.get(table_name)\n",
    "    fk_cols = fk_dict.get(table_name)\n",
    "    # Set PK column to NOT NULL\n",
    "    if pk_col and pk_col in df.columns:\n",
    "        spark.sql(f\"ALTER TABLE workspace.`tegge-insurance-data`.{table_name} ALTER COLUMN {pk_col} SET NOT NULL\")\n",
    "    # Set FK columns to NOT NULL\n",
    "    if fk_cols:\n",
    "        for fk_col in fk_cols:\n",
    "            if fk_col in df.columns:\n",
    "                spark.sql(f\"ALTER TABLE workspace.`tegge-insurance-data`.{table_name} ALTER COLUMN {fk_col} SET NOT NULL\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "health_ins_poc_full_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
